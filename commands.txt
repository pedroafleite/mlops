==COMPUTE ENGINE==
# access cloud shell, click SSH

use python 3.6 to 3.8
create a venv with the desired python version
conda create -n "venv" python=3.8.0 ipython
conda env list
conda activate venv

==PUBSUB==
gcloud auth login

export PROJECT_ID=project_id
gcloud projects create ${PROJECT_ID}

export SERVICE_ACCOUNT=service_account
gcloud iam service-accounts create ${SERVICE_ACCOUNT} --description="service account for mlops exercise" --display-name="service-mlops"
gcloud projects add-iam-policy-binding ${PROJECT_ID} --member="serviceAccount:${SERVICE_ACCOUNT}@${PROJECT_ID}.iam.gserviceaccount.com" --role="roles/pubsub.admin"
gcloud iam service-accounts list
gcloud iam service-accounts keys create key-file.json --iam-account=${SERVICE_ACCOUNT}@${PROJECT_ID}.iam.gserviceaccount.com

==PubSub to Storage using Dataflow==

gcloud compute regions list
gsutil mb gs://ml_input

gcloud projects add-iam-policy-binding mlops-1635587444840 --member="serviceAccount:service-mlops-1382162467@mlops-1635587444840.iam.gserviceaccount.com" --role="roles/storage.admin"
pip install apache_beam[gcp]
# allow your account to use apitools
python pubsub_gcs.py --project=mlops-1635587444840 --region=us-central1 --input_topic=projects/mlops-1635587444840/topics/topic-brigade --output_path=gs://ml_input/samples/output --runner=DataflowRunner --window_size=2 --num_shards=1 --temp_location=gs://ml_input/temp
python publisher.py
python subscriber.py

==DATAPROC==

export CLUSTER=cluster-name

gcloud dataproc clusters create ${CLUSTER} \
    --project=${PROJECT} \
    --region=${REGION} \
    --single-node
